{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479767765462_577245349","id":"20161122-073605_430595920","dateCreated":"2016-11-22T07:36:05+0900","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2474","text":"/*\r\n\tThis code is intended to be run in the Scala shell. \r\n\tLaunch the Scala Spark shell by running ./bin/spark-shell from the Spark directory.\r\n\tYou can enter each line in the shell and see the result immediately.\r\n\tThe expected output in the Spark console is presented as commented lines following the\r\n\trelevant code\r\n\r\n\tThe Scala shell creates a SparkContex variable available to us as 'sc'\r\n*/\r\n\r\n// sed 1d train.tsv > train_noheader.tsv\r\n// load raw data\r\nval rawData = sc.textFile(\"/PATH/train_noheader.tsv\")\r\nval records = rawData.map(line => line.split(\"\\t\"))\r\nrecords.first","dateUpdated":"2016-11-22T07:38:44+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479767913832_1289647071","id":"20161122-073833_527724590","dateCreated":"2016-11-22T07:38:33+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2615","text":"import org.apache.spark.mllib.regression.LabeledPoint\r\nimport org.apache.spark.mllib.linalg.Vectors\r\nval data = records.map { r =>\r\n\tval trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\r\n\tval label = trimmed(r.size - 1).toInt\r\n\tval features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\r\n\tLabeledPoint(label, Vectors.dense(features))\r\n}\r\ndata.cache","dateUpdated":"2016-11-22T07:40:11+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479767912525_1595522446","id":"20161122-073832_1195916963","dateCreated":"2016-11-22T07:38:32+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2545","text":"val numData = data.count\r\n// numData: Long = 7395\r\n// note that some of our data contains negative feature vaues. For naive Bayes we convert these to zeros\r\nval nbData = records.map { r =>\r\n\tval trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\r\n\tval label = trimmed(r.size - 1).toInt\r\n\tval features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d)\r\n\tLabeledPoint(label, Vectors.dense(features))\r\n}","dateUpdated":"2016-11-22T07:40:11+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479767998379_-608780623","id":"20161122-073958_1434281434","dateCreated":"2016-11-22T07:39:58+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2685","dateUpdated":"2016-11-22T07:40:42+0900","text":"// train a Logistic Regression model\r\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\r\nimport org.apache.spark.mllib.classification.SVMWithSGD\r\nimport org.apache.spark.mllib.classification.NaiveBayes\r\nimport org.apache.spark.mllib.tree.DecisionTree\r\nimport org.apache.spark.mllib.tree.configuration.Algo\r\nimport org.apache.spark.mllib.tree.impurity.Entropy\r\n\r\nval numIterations = 10\r\nval maxTreeDepth = 5\r\nval lrModel = LogisticRegressionWithSGD.train(data, numIterations)\r\nval svmModel = SVMWithSGD.train(data, numIterations)\r\n// note we use nbData here for the NaiveBayes model training\r\nval nbModel = NaiveBayes.train(nbData) \r\nval dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)\r\n\r\n// make prediction on a single data point\r\nval dataPoint = data.first\r\n// dataPoint: org.apache.spark.mllib.regression.LabeledPoint = LabeledPoint(0.0, [0.789131,2.055555556,0.676470588, ...\r\nval prediction = lrModel.predict(dataPoint.features)\r\n// prediction: Double = 1.0\r\nval trueLabel = dataPoint.label\r\n// trueLabel: Double = 0.0\r\nval predictions = lrModel.predict(data.map(lp => lp.features))\r\npredictions.take(5)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768001455_-649163934","id":"20161122-074001_1624291263","dateCreated":"2016-11-22T07:40:01+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2755","text":"// compute accuracy for logistic regression\r\nval lrTotalCorrect = data.map { point =>\r\n  if (lrModel.predict(point.features) == point.label) 1 else 0\r\n}.sum","dateUpdated":"2016-11-22T07:41:12+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768058797_2091261275","id":"20161122-074058_762748780","dateCreated":"2016-11-22T07:40:58+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2825","dateUpdated":"2016-11-22T07:41:31+0900","text":"// accuracy is the number of correctly classified examples (same as true label)\r\n// divided by the total number of examples\r\nval lrAccuracy = lrTotalCorrect / numData\r\n// lrAccuracy: Double = 0.5146720757268425\r\n\r\n// compute accuracy for the other models\r\nval svmTotalCorrect = data.map { point =>\r\n  if (svmModel.predict(point.features) == point.label) 1 else 0\r\n}.sum\r\nval nbTotalCorrect = nbData.map { point =>\r\n  if (nbModel.predict(point.features) == point.label) 1 else 0\r\n}.sum\r\n// decision tree threshold needs to be specified\r\nval dtTotalCorrect = data.map { point =>\r\n  val score = dtModel.predict(point.features)\r\n  val predicted = if (score > 0.5) 1 else 0 \r\n  if (predicted == point.label) 1 else 0\r\n}.sum\r\nval svmAccuracy = svmTotalCorrect / numData\r\n// svmAccuracy: Double = 0.5146720757268425\r\nval nbAccuracy = nbTotalCorrect / numData\r\n// nbAccuracy: Double = 0.5803921568627451\r\nval dtAccuracy = dtTotalCorrect / numData\r\n// dtAccuracy: Double = 0.6482758620689655"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768060659_-1527317455","id":"20161122-074100_2015187191","dateCreated":"2016-11-22T07:41:00+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2895","dateUpdated":"2016-11-22T07:41:46+0900","text":"// compute area under PR and ROC curves for each model\r\n// generate binary classification metrics\r\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\r\nval metrics = Seq(lrModel, svmModel).map { model => \r\n\tval scoreAndLabels = data.map { point =>\r\n  \t\t(model.predict(point.features), point.label)\r\n\t}\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\r\n}\r\n// again, we need to use the special nbData for the naive Bayes metrics \r\nval nbMetrics = Seq(nbModel).map{ model =>\r\n\tval scoreAndLabels = nbData.map { point =>\r\n  \t\tval score = model.predict(point.features)\r\n  \t\t(if (score > 0.5) 1.0 else 0.0, point.label)\r\n\t}\t\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\r\n}\r\n// here we need to compute for decision tree separately since it does \r\n// not implement the ClassificationModel interface\r\nval dtMetrics = Seq(dtModel).map{ model =>\r\n\tval scoreAndLabels = data.map { point =>\r\n  \t\tval score = model.predict(point.features)\r\n  \t\t(if (score > 0.5) 1.0 else 0.0, point.label)\r\n\t}\t\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\r\n}\r\nval allMetrics = metrics ++ nbMetrics ++ dtMetrics\r\nallMetrics.foreach{ case (m, pr, roc) => \r\n\tprintln(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\") \r\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768062607_-713188782","id":"20161122-074102_338234582","dateCreated":"2016-11-22T07:41:02+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2965","text":"// standardizing the numerical features\r\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\r\nval vectors = data.map(lp => lp.features)\r\nval matrix = new RowMatrix(vectors)\r\nval matrixSummary = matrix.computeColumnSummaryStatistics()\r\n\r\nprintln(matrixSummary.mean)\r\n// [0.41225805299526636,2.761823191986623,0.46823047328614004, ...\r\nprintln(matrixSummary.min)\r\n// [0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0, ...\r\nprintln(matrixSummary.max)\r\n// [0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444, ...\r\nprintln(matrixSummary.variance)\r\n// [0.1097424416755897,74.30082476809638,0.04126316989120246, ...\r\nprintln(matrixSummary.numNonzeros)\r\n// [5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0,7362.0, ...","dateUpdated":"2016-11-22T07:42:18+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768124506_1188163662","id":"20161122-074204_1212832960","dateCreated":"2016-11-22T07:42:04+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3035","dateUpdated":"2016-11-22T07:42:29+0900","text":"// scale the input features using MLlib's StandardScaler\r\nimport org.apache.spark.mllib.feature.StandardScaler\r\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\r\nval scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\r\n// compare the raw features with the scaled features\r\nprintln(data.first.features)\r\n// [0.789131,2.055555556,0.676470588,0.205882353,\r\nprintln(scaledData.first.features)\r\n// [1.1376439023494747,-0.08193556218743517,1.025134766284205,-0.0558631837375738,\r\nprintln((0.789131 - 0.41225805299526636)/math.sqrt(0.1097424416755897))\r\n // 1.137647336497682"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768128216_-1285402341","id":"20161122-074208_1553614506","dateCreated":"2016-11-22T07:42:08+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3175","text":"// train a logistic regression model on the scaled data, and compute metrics\r\nval lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\r\nval lrTotalCorrectScaled = scaledData.map { point =>\r\n  if (lrModelScaled.predict(point.features) == point.label) 1 else 0\r\n}.sum\r\nval lrAccuracyScaled = lrTotalCorrectScaled / numData\r\n// lrAccuracyScaled: Double = 0.6204192021636241\r\nval lrPredictionsVsTrue = scaledData.map { point => \r\n\t(lrModelScaled.predict(point.features), point.label) \r\n}\r\nval lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\r\nval lrPr = lrMetricsScaled.areaUnderPR\r\nval lrRoc = lrMetricsScaled.areaUnderROC\r\nprintln(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\") \r\n/*\r\nLogisticRegressionModel\r\nAccuracy: 62.0419%\r\nArea under PR: 72.7254%\r\nArea under ROC: 61.9663%\r\n*/","dateUpdated":"2016-11-22T07:42:49+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768128148_-1259239416","id":"20161122-074208_2108868316","dateCreated":"2016-11-22T07:42:08+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3105","text":"// Investigate the impact of adding in the 'category' feature\r\nval categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\r\n// categories: scala.collection.immutable.Map[String,Int] = Map(\"weather\" -> 0, \"sports\" -> 6, \r\n//\t\"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8,\r\n// \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \r\n// \"business\" -> 1, \"science_technology\" -> 7)\r\nval numCategories = categories.size\r\n// numCategories: Int = 14\r\nval dataCategories = records.map { r =>\r\n\tval trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\r\n\tval label = trimmed(r.size - 1).toInt\r\n\tval categoryIdx = categories(r(3))\r\n\tval categoryFeatures = Array.ofDim[Double](numCategories)\r\n\tcategoryFeatures(categoryIdx) = 1.0\r\n\tval otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\r\n\tval features = categoryFeatures ++ otherFeatures\r\n\tLabeledPoint(label, Vectors.dense(features))\r\n}\r\nprintln(dataCategories.first)\r\n// LabeledPoint(0.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,\r\n//\t0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,\r\n// 0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])","dateUpdated":"2016-11-22T07:43:09+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768179602_-925369316","id":"20161122-074259_823034673","dateCreated":"2016-11-22T07:42:59+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3245","text":"// standardize the feature vectors\r\nval scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))\r\nval scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features)))\r\nprintln(dataCategories.first.features)\r\n// [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,\r\n// 0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,\r\n// 5424.0,170.0,8.0,0.152941176,0.079129575]\r\nprintln(scaledDataCats.first.features)","dateUpdated":"2016-11-22T07:43:34+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768204449_-2106133042","id":"20161122-074324_1584757296","dateCreated":"2016-11-22T07:43:24+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3315","text":"// train model on scaled data and evaluate metrics\r\nval lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)\r\nval lrTotalCorrectScaledCats = scaledDataCats.map { point =>\r\n  if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\r\n}.sum\r\nval lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\r\nval lrPredictionsVsTrueCats = scaledDataCats.map { point => \r\n\t(lrModelScaledCats.predict(point.features), point.label) \r\n}\r\nval lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\r\nval lrPrCats = lrMetricsScaledCats.areaUnderPR\r\nval lrRocCats = lrMetricsScaledCats.areaUnderROC\r\nprintln(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\") \r\n/*\r\nLogisticRegressionModel\r\nAccuracy: 66.5720%\r\nArea under PR: 75.7964%\r\nArea under ROC: 66.5483%\r\n*/","dateUpdated":"2016-11-22T07:43:49+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768218897_358784595","id":"20161122-074338_1148151143","dateCreated":"2016-11-22T07:43:38+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3385","text":"// train naive Bayes model with only categorical data\r\nval dataNB = records.map { r =>\r\n\tval trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\r\n\tval label = trimmed(r.size - 1).toInt\r\n\tval categoryIdx = categories(r(3))\r\n\tval categoryFeatures = Array.ofDim[Double](numCategories)\r\n\tcategoryFeatures(categoryIdx) = 1.0\r\n\tLabeledPoint(label, Vectors.dense(categoryFeatures))\r\n}\r\nval nbModelCats = NaiveBayes.train(dataNB)\r\nval nbTotalCorrectCats = dataNB.map { point =>\r\n  if (nbModelCats.predict(point.features) == point.label) 1 else 0\r\n}.sum\r\nval nbAccuracyCats = nbTotalCorrectCats / numData\r\nval nbPredictionsVsTrueCats = dataNB.map { point => \r\n\t(nbModelCats.predict(point.features), point.label) \r\n}\r\nval nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\r\nval nbPrCats = nbMetricsCats.areaUnderPR\r\nval nbRocCats = nbMetricsCats.areaUnderROC\r\nprintln(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\") \r\n/*\r\nNaiveBayesModel\r\nAccuracy: 60.9601%\r\nArea under PR: 74.0522%\r\nArea under ROC: 60.5138%\r\n*/","dateUpdated":"2016-11-22T07:44:09+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768237358_1402269585","id":"20161122-074357_1321639742","dateCreated":"2016-11-22T07:43:57+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3455","dateUpdated":"2016-11-22T07:44:23+0900","text":"// investigate the impact of model parameters on performance\r\n// create a training function\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.mllib.optimization.Updater\r\nimport org.apache.spark.mllib.optimization.SimpleUpdater\r\nimport org.apache.spark.mllib.optimization.L1Updater\r\nimport org.apache.spark.mllib.optimization.SquaredL2Updater\r\nimport org.apache.spark.mllib.classification.ClassificationModel\r\n\r\n// helper function to train a logistic regresson model\r\ndef trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\r\n\tval lr = new LogisticRegressionWithSGD\r\n\tlr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\r\n\tlr.run(input)\r\n}\r\n// helper function to create AUC metric\r\ndef createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\r\n\tval scoreAndLabels = data.map { point =>\r\n  \t\t(model.predict(point.features), point.label)\r\n\t}\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(label, metrics.areaUnderROC)\r\n}\r\n\r\n// cache the data to increase speed of multiple runs agains the dataset\r\nscaledDataCats.cache"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768239607_-2071258810","id":"20161122-074359_410600395","dateCreated":"2016-11-22T07:43:59+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3525","text":"// num iterations\r\nval iterResults = Seq(1, 5, 10, 50).map { param =>\r\n\tval model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\r\n\tcreateMetrics(s\"$param iterations\", scaledDataCats, model)\r\n}\r\niterResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n1 iterations, AUC = 64.97%\r\n5 iterations, AUC = 66.62%\r\n10 iterations, AUC = 66.55%\r\n50 iterations, AUC = 66.81%\r\n*/","dateUpdated":"2016-11-22T07:44:43+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768271446_-1731864307","id":"20161122-074431_390794900","dateCreated":"2016-11-22T07:44:31+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3595","dateUpdated":"2016-11-22T07:45:04+0900","text":"// step size\r\nval stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\r\n\tval model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\r\n\tcreateMetrics(s\"$param step size\", scaledDataCats, model)\r\n}\r\nstepResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n0.001 step size, AUC = 64.95%\r\n0.01 step size, AUC = 65.00%\r\n0.1 step size, AUC = 65.52%\r\n1.0 step size, AUC = 66.55%\r\n10.0 step size, AUC = 61.92%\r\n*/"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768273620_-895420198","id":"20161122-074433_332280563","dateCreated":"2016-11-22T07:44:33+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3665","text":"// regularization\r\nval regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\r\n\tval model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\r\n\tcreateMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\r\n}\r\nregResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n0.001 L2 regularization parameter, AUC = 66.55%\r\n0.01 L2 regularization parameter, AUC = 66.55%\r\n0.1 L2 regularization parameter, AUC = 66.63%\r\n1.0 L2 regularization parameter, AUC = 66.04%\r\n10.0 L2 regularization parameter, AUC = 35.33%\r\n*/","dateUpdated":"2016-11-22T07:45:19+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768309698_-1671833612","id":"20161122-074509_1341023607","dateCreated":"2016-11-22T07:45:09+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3735","text":"// investigate decision tree\r\nimport org.apache.spark.mllib.tree.impurity.Impurity\r\nimport org.apache.spark.mllib.tree.impurity.Entropy\r\nimport org.apache.spark.mllib.tree.impurity.Gini\r\ndef trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\r\n\tDecisionTree.train(input, Algo.Classification, impurity, maxDepth)\r\n}\r\n \r\n// investigate tree depth impact for Entropy impurity\r\nval dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\r\n\tval model = trainDTWithParams(data, param, Entropy)\r\n\tval scoreAndLabels = data.map { point =>\r\n\t\tval score = model.predict(point.features)\r\n  \t\t(if (score > 0.5) 1.0 else 0.0, point.label)\r\n\t}\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(s\"$param tree depth\", metrics.areaUnderROC)\r\n}\r\ndtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n1 tree depth, AUC = 59.33%\r\n2 tree depth, AUC = 61.68%\r\n3 tree depth, AUC = 62.61%\r\n4 tree depth, AUC = 63.63%\r\n5 tree depth, AUC = 64.88%\r\n10 tree depth, AUC = 76.26%\r\n20 tree depth, AUC = 98.45%\r\n*/","dateUpdated":"2016-11-22T07:45:41+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768331459_1067624529","id":"20161122-074531_825682297","dateCreated":"2016-11-22T07:45:31+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3805","text":"// investigate tree depth impact for Gini impurity\r\nval dtResultsGini = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\r\n\tval model = trainDTWithParams(data, param, Gini)\r\n\tval scoreAndLabels = data.map { point =>\r\n\t\tval score = model.predict(point.features)\r\n  \t\t(if (score > 0.5) 1.0 else 0.0, point.label)\r\n\t}\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(s\"$param tree depth\", metrics.areaUnderROC)\r\n}\r\ndtResultsGini.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n1 tree depth, AUC = 59.33%\r\n2 tree depth, AUC = 61.68%\r\n3 tree depth, AUC = 62.61%\r\n4 tree depth, AUC = 63.63%\r\n5 tree depth, AUC = 64.89%\r\n10 tree depth, AUC = 78.37%\r\n20 tree depth, AUC = 98.87%\r\n*/","dateUpdated":"2016-11-22T07:45:59+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768349785_-1309907490","id":"20161122-074549_750383398","dateCreated":"2016-11-22T07:45:49+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3875","text":"// investigate Naive Bayes parameters\r\ndef trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\r\n\tval nb = new NaiveBayes\r\n\tnb.setLambda(lambda)\r\n\tnb.run(input)\r\n}\r\nval nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\r\n\tval model = trainNBWithParams(dataNB, param)\r\n\tval scoreAndLabels = dataNB.map { point =>\r\n  \t\t(model.predict(point.features), point.label)\r\n\t}\r\n\tval metrics = new BinaryClassificationMetrics(scoreAndLabels)\r\n\t(s\"$param lambda\", metrics.areaUnderROC)\r\n}\r\nnbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\r\n/*\r\n0.001 lambda, AUC = 60.51%\r\n0.01 lambda, AUC = 60.51%\r\n0.1 lambda, AUC = 60.51%\r\n1.0 lambda, AUC = 60.51%\r\n10.0 lambda, AUC = 60.51%\r\n*/","dateUpdated":"2016-11-22T07:46:28+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768376061_1131000829","id":"20161122-074616_1557155429","dateCreated":"2016-11-22T07:46:16+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3945","dateUpdated":"2016-11-22T07:46:47+0900","text":"// illustrate cross-validation\r\n// create a 60% / 40% train/test data split\r\nval trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\r\nval train = trainTestSplit(0)\r\nval test = trainTestSplit(1)\r\n// now we train our model using the 'train' dataset, and compute predictions on unseen 'test' data\r\n// in addition, we will evaluate the differing performance of regularization on training and test datasets\r\nval regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\r\n\tval model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\r\n\tcreateMetrics(s\"$param L2 regularization parameter\", test, model)\r\n}\r\nregResultsTest.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") }\r\n/*\r\n0.0 L2 regularization parameter, AUC = 66.480874%\r\n0.001 L2 regularization parameter, AUC = 66.480874%\r\n0.0025 L2 regularization parameter, AUC = 66.515027%\r\n0.005 L2 regularization parameter, AUC = 66.515027%\r\n0.01 L2 regularization parameter, AUC = 66.549180%\r\n*/"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768378166_1997070603","id":"20161122-074618_312507771","dateCreated":"2016-11-22T07:46:18+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4015","text":"// training set results\r\nval regResultsTrain = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\r\n\tval model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\r\n\tcreateMetrics(s\"$param L2 regularization parameter\", train, model)\r\n}\r\nregResultsTrain.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") }\r\n/*\r\n0.0 L2 regularization parameter, AUC = 66.260311%\r\n0.001 L2 regularization parameter, AUC = 66.260311%\r\n0.0025 L2 regularization parameter, AUC = 66.260311%\r\n0.005 L2 regularization parameter, AUC = 66.238294%\r\n0.01 L2 regularization parameter, AUC = 66.238294%\r\n*/","dateUpdated":"2016-11-22T07:46:58+0900"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479768397538_303806018","id":"20161122-074637_611012760","dateCreated":"2016-11-22T07:46:37+0900","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4085","text":"","dateUpdated":"2016-11-22T07:46:47+0900"}],"name":"Chapter 05","id":"2C39ZJM8F","angularObjects":{"2C47MGX9B:shared_process":[],"2C22X7YME:shared_process":[],"2C3K6RKJN:shared_process":[],"2C2EWZ9TA:shared_process":[],"2C2JMDYZ3:shared_process":[],"2C1S848VZ:shared_process":[],"2C2QVVB9Q:shared_process":[],"2C43GNJ9W:shared_process":[],"2C12D5W9R:shared_process":[],"2C1T2UZ3P:shared_process":[],"2C3G51W1E:shared_process":[],"2C36955YP:shared_process":[],"2BZTACRZS:shared_process":[],"2C4AVWZ7X:shared_process":[],"2C3MT2CTG:shared_process":[],"2C1T89CKC:shared_process":[],"2C33Z58NN:shared_process":[],"2C29H1CDW:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}